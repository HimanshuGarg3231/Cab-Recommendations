{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cab Service Recommendation System Design with Azure - A Data Engineering Perspective**\n",
    "\n",
    "### **Step 1: Data Ingestion (Producers)**\n",
    "1.\tA user books a cab through the App/UI.\n",
    "2.\tThe UI captures details such as ride ID, user ID, pickup and drop locations, timestamp, ride type, fare, and distance traveled.\n",
    "3.\tThese details are sent to Azure Event Hubs via an API.\n",
    "### **Step 2: Event Handling in Azure Event Hubs**\n",
    "1.\tEvent Hubs receives the booking data and temporarily stores it.\n",
    "2.\tIt acts as a message broker, capturing the events in partitions, ensuring that event messages are available for downstream processing.\n",
    "### **Step 3: Stream Processing & Storage**\n",
    "1.\tAzure Stream Analytics reads the data from Event Hubs in real-time.\n",
    "2.\tIt inserts the structured data into Azure SQL Database with additional transformation like formatting the timestamp column for partitioning.\n",
    "3.\tEach day’s data will be stored as a separate partition in SQL Table.\n",
    "4.\tThis partitioning enables efficient querying for recommendation process.\n",
    "5.\tThe processed data is stored and made available for further analysis.\n",
    "### **Step 4: Configure Azure Event Hubs**\n",
    "1.\tCreate an Event Hubs namespace and an Event Hub instance.\n",
    "2.\tSet up authentication for secure API access.\n",
    "3.\tDevelop an API (Flask, Node.js, etc.) to send booking data to Event Hubs.\n",
    "### **Step 5: Set Up Azure Stream Analytics**\n",
    "1.\tCreate a Stream Analytics job.\n",
    "2.\tConfigure Event Hubs as input.\n",
    "3.\tConfigure Azure SQL Database as output.\n",
    "4.\tUse the following query to insert data into SQL.\n",
    "5.\tEnsure Stream Analytics has permissions to write the input to SQL Database.\t\n",
    "### **Step 6: Verify Data Storage**\n",
    "1.\tQuery the CabBookings table to check if data is being stored.\n",
    "2.\tSELECT * FROM CabBookings;\n",
    "3.\tMonitor Event Hubs and Stream Analytics logs for troubleshooting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Schema of the Source Table (in Azure SQL Database)**\n",
    "Since the source data is partitioned by date, we will process a single day’s data at a time. This makes it easier to scale and manage large volumes of data, especially with 5 million rides per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Table Schema SQL\n",
    "CREATE TABLE source_rides (\n",
    "    rideId STRING,\n",
    "    userId STRING,\n",
    "    pickupLocation STRING,\n",
    "    dropLocation STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    rideType STRING,\n",
    "    fareAmount FLOAT,\n",
    "    distanceKm FLOAT\n",
    ")\n",
    "PARTITION BY RANGE (CAST(timestamp AS DATE));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Processing One Date at a Time:**\n",
    "We will read the ride data for a specific date using partitioning. PySpark will handle this efficiently on Azure Databricks.\n",
    "\n",
    "## **PySpark Script on Azure Databricks**\n",
    "We will create a PySpark script on Azure Databricks to process one date’s data from the source, generate the recommendations, and store the results in the Azure SQL Database.\n",
    "\n",
    "PySpark Script for Azure Databricks:\n",
    "Read the source data for a specific date (partitioned by date).\n",
    "Add necessary columns (ride_day, ride_hour, ride_period).\n",
    "Calculate the ride_count for each unique pickup-location → drop-location combination.\n",
    "Rank the recommendations for each user based on the ride count.\n",
    "Store the results in Azure SQL Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, dayofweek, hour, count, coalesce, lit, desc\n",
    "from pyspark.sql.window import Window, rank\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"RideRecommendationsProcessing\").getOrCreate()\n",
    "\n",
    "# Azure SQL Database Connection details\n",
    "jdbcUrl = \"jdbc:sqlserver://<server_name>.database.windows.net:1433;database=<database_name>\"\n",
    "connectionProperties = {\n",
    "    \"user\": \"<userName>\",\n",
    "    \"password\": \"<password>\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Read new rides from source table\n",
    "processingDate = datetime.now().date() - timedelta(days=1)\n",
    "newRides = f\"\"\"select * from sourceTable where CAST(timestamp AS DATE) = '{processingDate}\"\"\"\n",
    "\n",
    "# Read the source data partitioned by date (for a specific date)\n",
    "sourceRidesData = spark.read.jdbc(url = jdbcUrl, table = f\"({newRides}) as sourceData\", properties = connectionProperties)\n",
    "\n",
    "# Read the existing recommendations data\n",
    "existingRecommendations = spark.read.jdbc(url = jdbcUrl, table = \"recommendationTable\", properties = connectionProperties)\n",
    "\n",
    "\n",
    "# Create rideDay, rideHour, and ride_period columns\n",
    "processedRidesData = sourceRidesData.withColumn(\"rideDay\", dayofweek(col(\"timestamp\")))\\\n",
    "                    .withColumn(\"rideHour\", hour(col(\"timestamp\")))\\\n",
    "                    .withColumn(\"ride_period\", \n",
    "                                        when((col(\"rideDay\") <= 5) & (col(\"rideHour\").between(6, 10)), \"Weekday Commute\")\n",
    "                                        .when((col(\"rideDay\") <= 5) & (col(\"rideHour\").between(17, 21)), \"Weekday Commute\")\n",
    "                                        .when((col(\"rideDay\") <= 5), \"Weekday Routine\")\n",
    "                                        .when((col(\"rideHour\").between(21, 3)), \"Nightlife\")\n",
    "                                        .when((col(\"rideDay\") >= 6) & (col(\"rideHour\").between(10, 20)), \"Weekend Leisure\")\n",
    "                                        .when((col(\"rideDay\") >= 6) & (col(\"rideHour\").between(16, 23)), \"Weekend Social\")\n",
    "                                        .otherwise(\"Other\"))\n",
    "\n",
    "# Group by user_id, pickup_location, drop_location, ride_period and calculate ride_count\n",
    "newRidesCount = processedRidesData.groupBy(\"userId\", \"pickupLocation\", \"dropLocation\", \"ridePeriod\").agg(count(\"*\").alias(\"newRideCount\"))\n",
    "\n",
    "# Combine new rides count with existing recommendations\n",
    "combinedDf = existingRecommendations.join(newRidesCount, [\"userId\", \"pickUpLocation\", \"dropLocation\", \"ridePeriod\"], \"full_outer\").na.fill(0)\n",
    "\n",
    "# Update ride counts\n",
    "updatedDf = combinedDf.withColumn(\"ridesCount\", coalesce(col(\"rideCount\"), lit(0)) + col(\"newRideCount\")).drop(\"newRideCount\", \"rank\")\n",
    "\n",
    "# Rank the recommendations based on rideCount\n",
    "window_spec = Window.partitionBy(\"userId\").orderBy(desc(\"rideCount\"))\n",
    "finalRecommendations = updatedDf.withColumn(\"rank\", rank().over(window_spec)).filter(col(\"rank\") <= 10)  # Get top 10 recommendations\n",
    "\n",
    "# Save the result into the `recommendationTable` table in Azure SQL Database\n",
    "finalRecommendations.write.jdbc(url=jdbcUrl, table=\"recommendationTable\", mode=\"overwrite\", properties=connectionProperties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Elements in the Script:**\n",
    "1. Reading Data for a Single Date:\n",
    "> The data for a specific date (e.g., 2025-02-01) is read from Azure Data Lake Storage (ADLS) using the load method with the partition path for the specific date.\n",
    "2. Creating the rideDay, rideHour, ridePeriod Columns:\n",
    "> We use dayofweek and hour functions to extract the day and hour from the timestamp.\n",
    "> The ridePeriod is created using the when clause to classify the ride into commute, leisure, nightlife, etc.\n",
    "3. Calculating ride_count:\n",
    "> We group the data by userId, pickupLocation, dropLocation, and ridePeriod, and then count the number of rides in each group.\n",
    "4. Ranking Recommendations:\n",
    "> rank() is used to rank the recommendations for each user, based on the rideCount (frequent rides get a higher rank).\n",
    "> Only the top 10 recommendations are kept per user.\n",
    "5. Saving to Azure SQL Database:\n",
    "The processed and ranked recommendations are saved back to Azure SQL Database in the recommendationTable table using JDBC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Table Schema SQL\n",
    "CREATE TABLE recommendationTable (\n",
    "    userId STRING,                   -- Unique user identifier\n",
    "    rank INT,                        -- Rank of recommendation (1 to 10)\n",
    "    pickupLocation STRING,           -- derived pickup location\n",
    "    dropLocation STRING,             -- Suggested drop location\n",
    "    ridePeriod STRING,               -- Weekday Commute, Weekend Leisure, etc.\n",
    "    ridesCount INT,                  -- Frequency of this particular ride\n",
    "    PRIMARY KEY (userId, rank)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fetching Relevant Recommendations for UI**\n",
    "Once the recommendations table is populated, the UI will fetch recommendations based on the current location of the user and the current day/time.\n",
    "1. UI captures the current location and time.\n",
    "2. The backend fetches the top 10 recommendations based on the current location and filters out irrelevant recommendations (e.g., no commute rides on weekends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Query for Fetching Recommendations:\n",
    "SELECT pickupLocation, dropLocation \n",
    "FROM recommendationTable \n",
    "WHERE userId = 'U1' \n",
    "AND pickupLocation = 'Home'  # User's current location\n",
    "AND ridePeriod != 'Weekday Commute'  # Exclude commute rides on weekends if it is a weekend currently( caculated through current day)\n",
    "ORDER BY ridesCount DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Workflow Recap:**\n",
    "### **Data Ingestion:**\n",
    "Data for a single date is read from Azure Data Lake into Azure Databricks.\n",
    "\n",
    "### **Data Processing:**\n",
    "The PySpark script processes the data to generate ride day, ride hour, ride period and calculates the top 10 recommendations for each user.\n",
    "\n",
    "### **Storing Data:**\n",
    "The processed data is saved into the user_recommendations table in Azure SQL Database.\n",
    "\n",
    "### **Fetching Recommendations for UI:**\n",
    "The UI captures the current location and time, and fetches relevant recommendations from the user_recommendations table.\n",
    "\n",
    "This setup ensures that Azure Databricks is efficiently processing partitioned data, generating relevant recommendations for users, and storing them back into Azure SQL Database for fast retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
